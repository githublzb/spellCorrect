搜索引擎爬虫的基本需求和考核标准需要包含以下基本功能：


   （1）网站下载流速控制


   


    国内国外的搜索爬虫，科研机构爬虫数量很多，不同的站点抗抓取能力大相径庭，对网站的下载做好控制，避免将网站抓死。


   


   （2）网页抓全


   


    将互联网网页抓全，是极大的挑战，暗网暂且不提，就是明网抓全也不是容易的事情，新站发现，sitemap协议等用站长主动提交的支持等等。


   


   （3）网页抓新（更新及时性）


   


    网页总在不断变化中，如何当网页变化后（更新，消亡）能够及时更新，实时性和死链率等是表征这方面工作的重要指标。


   


   （4）网页重复抓取的避免


   


    为了及时捕捉网页的更新，对同一个网址必须经常去抓取，同样网络是一个网状结构，同一个网址可能被多次引用，这些都导致重复抓取的可能性，如果避免网页抓重，同时控制合理的更新频率，是非常关键的。


   


   （5）DNS自动解析


    


    如果抓取每个网页都进行一次DNS解析，那成本就太大了，维护一个DNS自动解析系统，可以大大降低域名服务器的负担，且大大提高效率。


   


   （6）镜像站点的识别


   


    网页内容相同，但域名不同的情况比比皆是，其中镜像站点的识别尤为关键


   


   （7）抓取的优先级调整


    


    抓取队列总是满的，周而复始，但在抓取的时候会出现，重要的，紧急的，不重要的，不紧急的内容，如何处理好排队的关系尤为重要，是单独开辟绿色通道，还是将其排队号前提都是需要细心打磨的。


   


   （8）抓取深度控制


    链接展开的深度控制，避免出现单个站点过分抓取，而使得其他站点持续饥饿


   


   （9）多爬虫的协作


    爬虫间的通行量要尽可能少，爬虫出现故障后的自动恢复，抓取主机的异地化等等，据说百度在国外部署的爬虫来抓取国外的站点。


   


   （10）网页下载的存储


    网页下载后的本地存储，链接提取，锚文本，链接关系的存储等等。


   


   （11）死链、跳转的识别和处理


    在抓取网页失败后，判断是死链还是当机，错误下载的网址再次抓取的时间间隔的控制，redirect的网页收集等等。


   


   考核标准


    （1）总有效的网页数（单机）


    （2）新站发现数（单机）


    （3）无效抓取的网页数（单机）


    （4）镜像站点数（单机）


    （5）全网站点的基本信息（更新周期，死链率，错误率）


    （6）重要网页的抓取及时性（随机抽取盲测）


    （7）抓取稳定性，故障率等