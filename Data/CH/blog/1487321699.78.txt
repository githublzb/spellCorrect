Compute or Memory(续）计算还是存储，在计算机进行实际运行时，是选择即时计算还是预先计算存储用时提取的方式？人类存储那么多信息为什么？海量数据怎么产生的？面对这些海量的数据产生了怎样的问题？


   


    我认为，人类存储信息一定具有这样两个特征：（1）重用性；（2）凭证性，首先是重用性，如果一个信息不需要重用，也没有必要存储，当然这种重用应该是多次，后一个凭证性，是我杜撰的，就是有些信息，可能永远不会重用，但也可能作为证据或者凭证，在需要的时候使用，这类信息的重用概率极低，但不可缺少，比如罪案现场可能会收集大量证据。


   


    信息的突然爆炸是由于人对信息的需求开始扩大，外设的增强，数码相机，手机等等；网络传播的发展，大量信息的复制，传播；现在可能有两个认知，一方面信息太少了，很多信息没有联网，这些信息在图书馆里，在人的大脑里，没有电子化，互联网化。另一方面信息太多了，重复信息太多，甚至各种信息真假难辨，搜索一个孕妇能不能喝茶，可能有多种答案。在翻译《深入搜索引擎》的时候，我接触了一个理论叫奥卡姆剃刀理论，就是能简洁的情况下，尽可能简洁，简洁或者说&ldquo;微&rdquo;这个概念，如今特别时髦，一方面，手机互联网的发展，信息需要更好的压缩（或者说简洁）可以在有限的屏幕下尽可能高效率的展示，另一方面，社会节奏加快，人对信息的需求更加急迫，很难耐心去看大段的内容，在沟通聊天中，经常采用表情符，和缩略语就是这种趋势的一个表现。绕了一大圈，再回来，信息的规模体量扩大，而人的需求是&ldquo;微&rdquo;的需求，这就需要通过智能的方法，来压缩信息，提炼出更加短小精干的信息。这应该是即搜索引擎之后又一个广阔的方向。当一个query搜索引擎返回了100个diversified and strong relevant的结果时，这种需求就显得愈发强烈。今天朱小燕老师的报告提到了俄罗斯潜艇爆炸事件，有1000多个关于该事实的描述的句子，如何抽取出最好的最简洁的描述该事件的句子。我想这个领域非常有意思。


   


    另外一个问题就是，当信息爆炸的时候，一个QA系统，是采用基于知识库的方式（memory+retrieval）的方式，还是基于（search+processing）的方式呢？今天朱小燕老师的报告也给我很多启发，采用知识库，本体的方式，结果是非常好的，但不在知识库的Query就没法回答，扩展的难度很大。而后者相当于先在海量的全部的数据中retrieval出相关的内容，然后进行精炼，加工处理的难度很大。我联想到我当初为了应付考研做高等数学的题目，早期只能memory下很多题目，当面对一个题目的时候，如果能retrieval的话，则很快能做出，如果不能就完蛋了，换句话说我memory的和这个题目得直标标的对上，但谁着做题的不断深入，产生了联想，引申的能力，可以做一些新题，产生一些新的解题思路，甚至我很难说出这个题目和我此前做的哪一个题目很像，直觉是一个新题，但有何此前做的题目有相关性，这时可能在大脑中，会有一个search的过程，找到一些此前相关的题解过程，通过processing产生了解题的思路。当然，我举得的这个例子可能有些牵强，从uncertainty到certainy的过程令人着迷，这也许是搞科研永远绕不过的障碍。


   


    为期两周的新生入学结束了，学校和计算机系请来了各种大师，内力深厚，非常崇拜，让我对科学，科研产生了新的认识和理解，让我从零开始，脚踏实地在一个新的领域上取得更大的突破，迎接来自星空的挑战。