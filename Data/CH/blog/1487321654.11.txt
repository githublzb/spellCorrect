关于Human-based computing（基于人的计算，人本计算）的一篇好文章转载自刘挺老师的博客：http://blog.sina.com.cn/s/blog_4cbec5e90100hd79.html


    近来，互联网领域概念纷飞，云计算(Cloud Computing)、服务计算(Service Computing)、物联网(Internet of Things)、CPS(Cyber-Physical System)、社会计算(Social Computing)、智慧地球(Smart Earth)等等，看的人&#30524;花缭乱。


   


    我不认为上述概念是企业界的炒作，无风不起浪，这些新名词的频繁出现预示着一次新的互联网革命的到来。作为信息检索和自然语言处理领域的研究者，我像一只焦躁不安的海鸟，预感到大潮的来临，迫切地想知道这次狂潮会给我脚下的这片岛屿带来什么。


    经过多日的苦思冥想，我确认自己在恍恍惚惚中悟出了一点儿道理，搜肠刮肚，找不到一个合适的词语来表达，且借时下有点儿时髦，却又不甚火爆的“人本计算”来阐述我的想法吧。


   


    所为人本计算，英文可以翻译为Human-based(centered) Computing，简写为HC，含义是“基于人、以人为中心的计算”，一看便知包括两个方面：“基于人的计算（以人为本源）”(Human-Based Computation，HBC)和“以人为中心的计算（以人为本位）”(Human-Centered Computing，HCC)。我们先说基于人的计算。


   


    知识获取是一切智能系统的关键，让我们从这个角度回顾一下语言技术的发展。第一代语言技术是专家系统式的，其知识获取形式是语言学家的语言学专业知识加上计算机专家的算法知识，这些知识是封闭的，有限的，以封闭而有限的知识面对开放而无限的语言现象，当然很快就败下阵来。第二代语言技术是基于和面向大规模真实文本的，广泛采用统计方法，语言学家插不上手，知识获取的来源是大规模原始语料和小规模经过标注人员手工标注的深加工语料。真实文本蕴含着用户提供的知识，但不经标注的原始纯文本只能提供很浅的不精确的知识，而组织一些学生对语料进行标注的工作每每令研究者备尝艰苦，几万句的树库搞搞研究尚可，面向不同领域的实际应用就差得远了。第三代语言技术就是人本计算，即基于人的计算，有大规模用户参与的计算，下面开始阐述。


    当我们把互联网理解为一群机器的网络时，我们把用户排斥在外了，而Web 2.0时代最大的特点就是用户的参与。我们搞自然语言处理的人，最喜欢纯文本，处理网页时第一个动作就是提取正文，费好大的气力把我们最亲切的纯文本挖出来，各种tag标记全都去掉。这就好比日本兵拼刺刀，一定要先推掉子弹，这样的做法是自己难为自己，或者说是蒙上&#30524;睛不去看新形势下一切对我有利的条件，仍按老办法行事。当我们换一个观念，把互联网上每台电脑背后的用户也算作互联网的一部分时，我们惊讶地发现，陈旧的方法损失掉了太多有价&#20540;的信息。


   这些有价&#20540;的信息都是人提供的，包括：&#26684;式、链接、网络结构、用户行为、社会化标签、发帖时间、网址等。网页&#26684;式的解析最让人感觉枯燥乏味，但其实&#26684;式是信息发出者向信息接收者主动透漏的元数据，比如专名加黑，专名是超链接，这可以有效地帮助我们识别命名实体。链接上的文字是对其指向的那个页面的描述。社会化标签就更有用了，比如自然语言处理领域非常棘手的“别名、简称”问题，以前几乎是一筹莫展，现在有了新的解决方案，可以从不同用户对同一篇博文或同一张照片给出的不同标签词中发现别名和简称。至于用户的行为则向我们展示出更多的蛛丝马迹，用户输入一个查询，继而点击了搜索结果中的某个网页，则我们可以认为这个查询与这个网页具有相关性。用户查询连续发出的一组相关查询称为Session，我们看到当前搜索引擎提供的相关查询服务正是利用Session实现的，而不是人工编制的同义词词典。通过发现兴趣相&#20284;的用户，当用户浏览新闻时，可以看到其他相&#20284;用户看过的新闻。PageRank，浏览数、回帖数等等都是页面价&#20540;和热点话题计算的重要依据。总之，除了文本内容以外，互联网上围绕内容透露着非常丰富的信息，这为语言技术提供了新的知识源泉。


   上面描述的还只是用户在不经意间提供的信息描述，除此之外，互联网上已经涌现出更多人人协同工作的模式。最典型的是维基（百科）、知道，这些互动产品，用户的动力在于自我表现，他们在自我表现的过程中挖掘出丰富的常识知识和行业知识。人工智能最缺少的就是常识知识，从Wiki这种半结构化的文本中获取常识知识，比直接由人工编写常识知识库要省力得多。此外，搜狗拼音输入法是一个非常好的例子，借助网友贡献的词汇，使输入的速度和便捷性迈上了一个新的台阶。


    要让人更多地贡献知识，需要对他们进行激励，除了激发用户的自我表现欲外，游戏是非常重要的一种激励手段。CMU的Luis Von Ahn发明了“人计算”(Human Computation)，这种思想的核心是变“人让机器算”为“机器让人算”，通过设计各种游戏，让用户在娱乐中为机器贡献标注数据，比如给两个互不认识的网友同时看一张图片，如果两人给出了相同的关键词，就都可以得分，而这个关键词可以作为该图片的正确标注保存起来。Luis关于人计算的论文在Science上发表，这让我们更认清了到底什么叫“创新”，创新不是一定要把简单的问题复杂化，只要是巧思独运，切实解决问题，就是创新。


    从用户那里获取知识，还有一个重要的特点，那就是Lifelong的学习。以往我们要在训练阶段把机器的学习任务完成，在使用阶段机器不再进化。而在基于人的计算中，用户一边用系统，一边教系统，以个性化新闻推送为例，用户用得越多，系统就越了解用户，变得越聪明。


   


    再来说说人本计算的另一个侧面“以人为中心的计算(HCC)”。“基于人的计算(HBC)”谈的是从人（用户）那里获取知识的问题，而HCC谈的是面向人的应用模式设计问题。


    一直以来，我们有一个误区，就是要把“机器智能”推向极致，把人的知识、人的智能赋予机器，让机器代替人脑去工作，人可以“垂手而天下治”。但反思多年来能够走向应用的智能技术，却发现他们无一例外不是以人机交互的方式在开展工作，而全自动的智能技术却始终无法走向市场。


    这样的例子很多。全自动机器翻译在帮人写英文、翻译英文上用不起来，在浏览英文新闻时可以用，但很少有人用，相反辅助翻译，包括翻译记忆却产生了很大的市场价&#20540;。全自动的开放域问答系统已经有10年左右的时间，没有见到独立的应用，问答和检索的区别在于，检索是找出一些参考文献来，对不对由用户来判断，用户觉得检索结果都不满意，还可以修改查询再次检索，所以检索是交互式的，是人机协同的信息查找过程，而问答要解决问题，返还给用户的答案需要全面准确而没有冗余，这就难了，用户不看信息出处，无法判定答案是否正确。问答系统的准确率无法达到100%，所以不提供交互方式，问答系统就永远无法走向实用。因此，基于FAQ的问答是一条出路，而受限域问答不管受限到多小的领域也没法用。还有一个是听写机（全自动语音识别），到现在也没有看到应用的曙光，相反，拼音输入法是成功的，因为人可以对拼汉转换的错误进行修改，可以让机器记住自己经常输入的词语。


    由此，我得出一个结论：在可预见的未来，智能技术的应用模式必须是人机交互，人机协同，或者是在机器的支持下的人人协同，而不会是人发命令，机器全自动完成。我们必须放弃“全自动”的理想，而对看上去不那么完美的“人机协同”满怀激情，让计算的负载在人脑和电脑之间求得平衡。电脑的优势在于快速低成本地对信息进行存储、复制、传输、比较、排序和检索，人脑的优势在于联想、推理、分析、归纳，而“群脑”的优势在于智慧的互补，所谓三个臭皮匠赛过诸葛亮。我们的目标不应再是追求电脑的智能极致，那是舍本逐末，好比本来要钓鱼，鱼竿只是手段，后来却迷上了鱼竿，开了鱼竿加工厂，我们的目标应该是追求提高人类的生产效率和生活体验。


    从无用户参与，到人机协同，再到以网络为媒介，以计算为助力的人人协同，是由一个飞跃。在Web 2.0时代，这样的成功应用案例不胜枚举。自动问答系统不成功，但百度知道这样的社区型问答却很成功，计算技术在此帮助用户整理问答对，检索问答对，把问题推送给合适的人，起到穿针引线的作用，但不是唱主角。搜索引擎中的查询推荐，当当上的图书推荐，以至于双语例句检索和基于实例的机器翻译背后都是人人协同的理念，看看别人是怎么翻译的，我照猫画虎也能翻译。


    除了用户自愿进行的人人协同工作外，还有付费形式的协同工作。比如“威客”，用户设定一个价&#26684;，要求其他网友为他设计一个logo，这是一种知识交易。联想开去，你帮我设计logo，我帮你做奥数题，还有中外用户互相帮助修改“外语”作文等等，这是一个“集体智慧(Collective Intelligence)”大发展的时代，我们设计新一代的互联网智能应用系统，必须把技术的姿态放低，充分地自然地辅助用户去完成他个人的任务，去与其他用户协同完成更复杂的任务。


    在人类的各种通讯形式中，“多人远距离异步通讯”是最难的，互联网只为其他通讯形式提高了速度，降低了成本，比如即时通讯比电话的成本更低，电子邮件比传统的信件更快等，但“多人远距离异步通讯”以往几乎没有，最多看到领导之间由通讯员传递文件，依次批示。而在Web 2.0时代，最发达的就是这类通讯，包括：Wiki、论坛、贴吧、博客、微博、邮件组、知道等各种各样丰富多彩的形式。


    至此，我又讲完了我关于“以人为中心的计算”的一些感悟。


   


    我还要补充说明的是，作为信息检索与自然语言处理领域的研究者，我们的目标仍然是“搜索信息，理解语言，挖掘知识”，这种目标和使命并没有因为人本计算理念的影响而改变，但在Web 2.0时代，我们需要以人本计算的观念调整完善我们完成这些使命的方法，核心是重视人的参与。以往没有充分重视的一些研究点必须重视起来，比如版面分析、用户日志挖掘、人机交互界面设计、lifelong的机器学习、UGC（用户产生的内容，User Generated Content）的分析、基于游戏的知识采集、基于社会化标签的别名识别、基于FAQ的问答系统、个性化新闻推荐、协同式机器翻译等等。研究社会计算的学者可能利用用户日志走向社会关系网络，甚至人工社会等研究方向，但我们应该限制自己，坚持“利用语言技术帮助人们从互联网上获取知识”的宗旨，把我们获取到的多维信息（行为、结构、&#26684;式等）用于处理最初的那一维（内容）。此外，并不是说有了“人本计算”，第一代的专家知识加算法，第二代的专业人员语料库标注加统计学习的方法就都废弃，相反，他们仍然是需要的，我们有了大炮，但也需要匕首和步枪。


   


     做个总结：“人本计算”就是“（知识）从群众中来，到群众中去（为群众服务）”，“一切依靠群众（提供知识），一切为了群众（获得知识服务）”，用英文说是“from the people, for the people”，人与机器共建“和谐”。计算技术发展到今天，从以计算为中心（比如，曾有人认为机器语言就够了，不用设计高级语言，现在也有人建议设计规范的查询语言而不是让用户自由输入查询），到以数据为中心（有人认为没有人工智能，只有数据，还有所谓“内容为王”的提法），到今天“以人为中心”，这是人本主义的“王者归来”，我们不会被自己制造的机器所奴役和压迫，神不是本，物也不是本，“人”才是本，机器只能向人学习，为人服务。


   


    我曾在10年前完整地提出了类&#20284;“百度知道”平台的一揽子设计方案，当时命名为“全球信息交换GIE(Globle Information Exchange)”，但此后却没有真正重视互联网上的用户。Web 2.0的概念提出来也已经好几年了，我身在互联网圈里，却没有从中得到灵感，一直把自己现在文本检索上，而没有做互联网检索，实在是后知后觉。今年是哈工大信息检索研究中心成立10年（2000年9月成立），这几天我尽力排除各种事务的干扰，比较安心地思考着未来的研究计划，才有所醒悟，写出上面的文字，希望对各位读者有所帮助。